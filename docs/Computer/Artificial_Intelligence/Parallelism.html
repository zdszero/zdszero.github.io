<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    inference
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      <style type="text/css">
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
    </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">inference</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#parallelism"
          id="toc-parallelism">Parallelism</a>
          <ul>
          <li><a href="#model-parallelism"
          id="toc-model-parallelism">Model Parallelism</a></li>
          <li><a href="#dp" id="toc-dp">DP</a></li>
          <li><a href="#tp" id="toc-tp">TP</a></li>
          <li><a href="#pp" id="toc-pp">PP</a></li>
          <li><a href="#ep" id="toc-ep">EP</a></li>
          </ul></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h2 id="parallelism">Parallelism</h2>
                <table>
                <colgroup>
                <col style="width: 20%" />
                <col style="width: 13%" />
                <col style="width: 17%" />
                <col style="width: 15%" />
                <col style="width: 20%" />
                <col style="width: 13%" />
                </colgroup>
                <thead>
                <tr>
                <th>Parallelism</th>
                <th>batch dim</th>
                <th>sequence dim</th>
                <th>hidden dim</th>
                <th>weights</th>
                <th>optimizer</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>DP</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                </tr>
                <tr>
                <td>TP</td>
                <td></td>
                <td></td>
                <td>√</td>
                <td>√(intra-layer)</td>
                <td></td>
                </tr>
                <tr>
                <td>Ring Attention</td>
                <td></td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                </tr>
                <tr>
                <td>PP</td>
                <td></td>
                <td></td>
                <td></td>
                <td>√(intra-layer)</td>
                <td>√</td>
                </tr>
                <tr>
                <td>ZeRO</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                <td>√</td>
                </tr>
                <tr>
                <td>FSDP</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td>√(intra-layer)</td>
                <td>√</td>
                </tr>
                </tbody>
                </table>
                <h3 id="model-parallelism">Model Parallelism</h3>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-09-17-22-43.png"
                width="500" alt="intra vs inter model parallelism" />
                <figcaption aria-hidden="true">intra vs inter model
                parallelism</figcaption>
                </figure>
                <ul>
                <li><strong>Intra-Operator</strong>: partitions
                computationally intensive operators, such as matrix
                multiplications, across multiple GPUs, accelerating
                computation but causing substantial communication.</li>
                <li><strong>Inter-Operator</strong>: organizes LLM
                layers into stages, each running on a GPU to form
                pipelines.</li>
                </ul>
                <h3 id="dp">DP</h3>
                <p>直接部署多份服务，将数据拆分到不同的服务上。</p>
                <h3 id="tp">TP</h3>
                <p>将矩阵运算拆分到多个 GPU 上，运算完之后进行 AllGather
                或者 AllReduce 的操作。AllGather 适用于 Column-wise
                sharding，AllReduce 适用于 Row-wise sharding。</p>
                <p>对于矩阵乘法 <span class="math inline">\(A \times
                B\)</span></p>
                <h4 id="列拆分">列拆分</h4>
                <p>Column-wise sharding 将 <span
                class="math inline">\(B\)</span> 拆分为 <span
                class="math inline">\(B_{0}, B_{1}\)</span></p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-02-07-16-02-02.png"
                width="500" alt="Column-wise sharding" />
                <figcaption aria-hidden="true">Column-wise
                sharding</figcaption>
                </figure>
                <p>对结果进行 AllGather 操作，every shard calculates the
                partial target martix.</p>
                <h4 id="行拆分">行拆分</h4>
                <figure>
                <img
                src="../.././WikiImage/image_2025-02-07-16-02-31.png"
                width="500" alt="Row-wise sharding" />
                <figcaption aria-hidden="true">Row-wise
                sharding</figcaption>
                </figure>
                <p>对结果进行 AllReduce 操作，every shard calculates the
                partial result of the whole target matrix.</p>
                <div class="sourceCode" id="cb1"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Non-split matrices used for verification.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.rand(<span class="dv">16</span>, <span class="dv">32</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.rand(<span class="dv">32</span>, <span class="dv">8</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> A <span class="op">@</span> B</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split A along columns, B along rows, then:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU 1: A1, B1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU 2: A2, B2</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>A1, A2 <span class="op">=</span> A.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>B1, B2 <span class="op">=</span> B.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>C1 <span class="op">=</span> A1 <span class="op">@</span> B1</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>C2 <span class="op">=</span> A2 <span class="op">@</span> B2</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that the result is the same after an element-wise sum.</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>torch.testing.assert_close(C, C1 <span class="op">+</span> C2)</span></code></pre></div>
                <h3 id="pp">PP</h3>
                <p>the model is split by layer into several chunks, each
                chunk is given to a device</p>
                <p>During the forward pass, each device passes the
                intermediate activation to the next stage. During the
                backward pass, each device passes the gradient of the
                input tensor back to the previous pipeline stage.</p>
                <p>Device with PP operates on
                <strong>mirco-batch</strong> split by stages.</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-28-01.png"
                width="500" alt="gpipe" />
                <figcaption aria-hidden="true">gpipe</figcaption>
                </figure>
                <h3 id="ep">EP</h3>
                <p>Expert Parallism</p>
                <hr />
                <p><a
                href="https://colossalai.org/docs/concepts/paradigms_of_parallelism/">Paradigm
                of Parallism</a></p>
                <p><a
                href="https://robotchinwag.com/posts/demystifying-tensor-parallelism/">Demystifying
                Tensor Parallelism</a></p>
              </div>
    </div>
  </div>
</body>

</html>
