<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    KV Cache
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">KV Cache</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div class="span12">
                <p><strong>Why no Q Cache?</strong></p>
                <p>Q, K, V are high dimension vector used to encode each
                token in vocabulary.</p>
                <p><span class="math inline">\(Q = \begin{bmatrix} q_1
                \\q_2 \\ \vdots \\ q_n \end{bmatrix}\)</span> <span
                class="math inline">\(K = \begin{bmatrix} k_1 \\k_2 \\
                \vdots \\ k_n \end{bmatrix}\)</span> <span
                class="math inline">\(V = \begin{bmatrix} v_1 \\v_2 \\
                \vdots \\ v_n \end{bmatrix}\)</span></p>
                <p><span class="math inline">\(k_i, q_i \in R^{1 \times
                d_k}, v_{i} \in R^{1 \times d_v}\)</span></p>
                <hr />
                <p>The core of transoformer mechanism is attention,
                where we calcuate the attention of current token with
                all its previous tokens.</p>
                <p><span class="math inline">\(Q \cdot K^{T} =
                \begin{bmatrix} q_1 \\q_2 \\ \vdots \\ q_n \end{bmatrix}
                \begin{bmatrix} k_1^{T} &amp; k_2^{T} &amp; \cdots &amp;
                k_n^{T} \end{bmatrix} =
                \begin{bmatrix}
                q_1 \cdot k_1^{T} &amp; q_1 \cdot k_2^{T} &amp; \cdots
                &amp; q_1 \cdot k_n^{T} \\
                q_2 \cdot k_1^{T} &amp; q_2 \cdot k_2^{T} &amp; \cdots
                &amp; q_2 \cdot k_n^{T} \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                q_n \cdot k_1^{T} &amp; q_n \cdot k_2^{T} &amp; \cdots
                &amp; q_n \cdot k_n^{T}
                \end{bmatrix}\)</span></p>
                <p>Without casual mask:</p>
                <p><span class="math inline">\(S(Q K^{T})V =
                \begin{bmatrix}
                S(q_1 \cdot k_1^{T}) &amp; S(q_1 \cdot k_2^{T}) &amp;
                \cdots &amp; S(q_1 \cdot k_n^{T}) \\
                S(q_2 \cdot k_1^{T}) &amp; S(q_2 \cdot k_2^{T}) &amp;
                \cdots &amp; S(q_2 \cdot k_n^{T}) \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                S(q_n \cdot k_1^{T}) &amp; S(q_n \cdot k_2^{T}) &amp;
                \cdots &amp; S(q_n \cdot k_n^{T})
                \end{bmatrix} \begin{bmatrix}
                v1 \\ v2 \\ \vdots \\ v_n
                \end{bmatrix} = \begin{bmatrix}
                S(q_1 \cdot k_1^{T})v_1 + S(q_1 \cdot k_2^{T})v_2 +
                \cdots + S(q_1 \cdot k_n^{T})v_n \\
                S(q_2 \cdot k_1^{T})v_1 + S(q_2 \cdot k_2^{T})v_2 +
                \cdots + S(q_2 \cdot k_n^{T})v_n \\
                \vdots \\
                S(q_n \cdot k_1^{T})v_1 + S(q_n \cdot k_2^{T})v_2 +
                \cdots + S(q_n \cdot k_n^{T})v_n
                \end{bmatrix}\)</span></p>
                <p>So this matrix is quietly verbose, because the
                attention computation is incremental and
                auto-regressive. Which means everytime we only care
                about the last token.</p>
                <p>So we could optimize this process with causal
                mask.</p>
                <p><span class="math inline">\(S(Q K^{T})V =
                \begin{bmatrix}
                S(q_1 \cdot k_1^{T}) &amp; 0 &amp; \cdots &amp; 0 \\
                S(q_2 \cdot k_1^{T}) &amp; S(q_2 \cdot k_2^{T}) &amp;
                \cdots &amp; 0 \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                S(q_n \cdot k_1^{T}) &amp; S(q_n \cdot k_2^{T}) &amp;
                \cdots &amp; S(q_n \cdot k_n^{T})
                \end{bmatrix} \begin{bmatrix}
                v1 \\ v2 \\ \vdots \\ v_n
                \end{bmatrix} = \begin{bmatrix}
                S(q_1 \cdot k_1^{T})v_1 \\
                S(q_2 \cdot k_1^{T})v_1 + S(q_2 \cdot k_2^{T})v_2 \\
                \vdots \\
                S(q_n \cdot k_1^{T})v_1 + S(q_n \cdot k_2^{T})v_2 +
                \cdots + S(q_n \cdot k_n^{T})v_n
                \end{bmatrix}\)</span></p>
                <p>For auto-regressive process, to calculate the last
                tokenâ€™s attention:</p>
                <p><span class="math inline">\(\text{attn} = S(q_n \cdot
                k_1^{T})v_1 + S(q_n \cdot k_2^{T})v_2 + \cdots + S(q_n
                \cdot k_n^{T})v_n\)</span></p>
                <hr />
                <p>VLLM</p>
                <ul>
                <li><code>execute_model(input_ids, input_positions, kv_caches, input_metadata)</code>
                <ul>
                <li><code>input_ids</code>: the last token of each
                request in a batch</li>
                <li><code>input_positions</code>: index of
                input_ids</li>
                <li><code>input_metadata</code>: other information</li>
                </ul></li>
                </ul>
              </div>
    </div>
  </div>
</body>

</html>
