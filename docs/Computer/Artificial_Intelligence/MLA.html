<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    MLA
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      <style type="text/css">
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
    </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">MLA</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#mla" id="toc-mla">MLA</a></li>
          <li><a href="#rope" id="toc-rope">Rope</a></li>
          <li><a href="#absorb" id="toc-absorb">Absorb</a></li>
          <li><a href="#analysis" id="toc-analysis">Analysis</a></li>
          <li><a href="#code" id="toc-code">Code</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h3 id="mla">MLA</h3>
                <figure>
                <img src="../.././WikiImage/mla_naive_raw.png"
                alt="mla_naive_raw" />
                <figcaption
                aria-hidden="true">mla_naive_raw</figcaption>
                </figure>
                <figure>
                <img
                src="../.././WikiImage/mla_naive_details.drawio.svg"
                alt="mla naive details" />
                <figcaption aria-hidden="true">mla naive
                details</figcaption>
                </figure>
                <h3 id="rope">Rope</h3>
                <p><strong>为什么 rope 和 nope 分离？</strong></p>
                <p>首先 rope 的旋转矩阵无法被融合进入 W_q 和 k_b
                矩阵之中（该矩阵并不是一个常量）：</p>
                <p><span class="math display">\[
                q_t^{(s)} {k_i^{(s)}}^\top
                = \left( x_t W_q^{(s)} \mathcal{R}_t \right)
                  \left( c_i W_k^{(s)} \mathcal{R}_i \right)^\top
                = x_t \left( W_q^{(s)} \mathcal{R}_{t-i}
                {W_k^{(s)}}^\top \right) c_i^\top
                \]</span></p>
                <p>既然无法融合，在使用 MLA 的情况下如果需要使用
                rope，就需要升维之后重新计算，但这样的话需要在升维后再重新计算
                rope，它也不想，所以干脆把 q 和 k
                拆成两部分，一部分是位置编码的，一部分是无关位置编码的。带位置编码的
                k 可以缓存，比缓存全部维度都带位置的 k 强。</p>
                <h3 id="absorb">Absorb</h3>
                <p><strong>normal 和 absorb 的区别？</strong></p>
                <p>absorb 就是把 wkv_b 拆分成 wk_b_nope 和
                wv_b，然后分别合入到 wq_b_nope 和 o_proj 中。</p>
                <p><span class="math display">\[\mathbf{q}_t^\top
                \mathbf{k}_j^C = (W^Q \mathbf{h}_t)^\top W^{UK}
                \mathbf{c}_j^{KV} = \mathbf{h}_t^\top (W^{Q^\top})
                W^{UK} \mathbf{c}_j^{KV} = \mathbf{h}_t^\top W^{Q^\top}
                W^{UK} \mathbf{c}_j^{KV} = \mathbf{h}_t^\top W^{Q^\top
                UK} \mathbf{c}_j^{KV}\]</span></p>
                <p><span class="math display">\[\mathbf{u}_t = W^O
                \mathbf{o}_t = W^{O} W^{UV} \sum_{j=1}^t
                \text{Softmax}_j \left( \frac{\mathbf{q}_t^\top
                \mathbf{k}_j}{\sqrt{d_h}} \right) \mathbf{c}_j^{KV} =
                W^{OUV} \sum_{j=1}^t \text{Softmax} \left(
                \frac{\mathbf{q}_t^\top \mathbf{k}_j^{KV}}{\sqrt{d_h}}
                \right) \mathbf{c}_j^{KV}\]</span></p>
                <p>也就是将 wq_b_nope 和 wk_b_nope 进行一个融合，wv_b 和
                o_proj 进行一个融合。融合之后实际上就相当于 kv_latent 和
                absorb 之后的 q 每个 head 做 MQA。</p>
                <p>此时 kv_latent 的 dim 为 (b, t, c)，q_absorb 的 dim
                为 (b, s, h, c)，两者相乘实际上实在 lora_rank
                上做向量内积，所以乘法结果 scores 的 dim 为 (b, t, s,
                h)，然后再将 scores 和 kv_latent 相乘，实际上实在 kv_len
                上做内积，所以结果的 dim 为 (b, s, h,
                c)，可以发现，此时结果的最后一个维度是 c 而不是
                h<em>d，但是我们已经将 o_proj 和 wv_b
                融合了，融合后矩阵的 dim 为 (b, hd, hidden_size) </em>
                (b, c, hd) → (b, c,
                hidden_size)，所以这么一乘最后还是能够得到
                hidden_size。</p>
                <figure>
                <img src="../.././WikiImage/mla_absorb.drawio.svg"
                alt="mla absorb" />
                <figcaption aria-hidden="true">mla absorb</figcaption>
                </figure>
                <h3 id="analysis">Analysis</h3>
                <p>为了方便，这里不考虑 gemm 的乘 2，以及 softmax，乘以
                factor 这种</p>
                <p><strong>absorb 在 decode
                可以减少计算量的本质原因</strong></p>
                <p>absorb 之后做 k_b 和 v_b 矩阵乘法的时候是 q_len
                为尺度，而不是以 kv_len
                为尺度，所以这一部分减少了很多计算量。</p>
                <p>absorb 之后 mqa 由于 head_dim 从 192 变成了
                576，所以这一部分计算量是变高了，但是 k_v 和 v_b
                减少的计算量完全可以抵消这一部分。</p>
                <p>对于 prefill，attention
                阶段的计算量增加太多，无法被抵消。</p>
                <div class="sourceCode" id="cb1"><pre
                class="sourceCode py"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flops_vanilla(q_len, kv_len, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    all_ops <span class="op">=</span> {</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_q_a&#39;</span>: q_len <span class="op">*</span> hidden_size <span class="op">*</span> q_lora_rank, <span class="co"># from Q to c_q</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_kv_a&#39;</span>: kv_len <span class="op">*</span> hidden_size <span class="op">*</span> (kv_lora_rank <span class="op">+</span> qk_rope_head_dim), <span class="co"># from KV to c_kv and k_pe</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_q_b&#39;</span>: q_len <span class="op">*</span> q_lora_rank <span class="op">*</span> n_heads <span class="op">*</span> qk_head_dim, <span class="co"># from c_q to q_nope and q_pe</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_k_b&#39;</span>: kv_len <span class="op">*</span> kv_lora_rank <span class="op">*</span> n_heads <span class="op">*</span> qk_nope_head_dim, <span class="co"># from c_kv to k_nope</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_v_b&#39;</span>: kv_len <span class="op">*</span> kv_lora_rank <span class="op">*</span> n_heads <span class="op">*</span> v_head_dim, <span class="co"># from c_kv to v_dim</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_mha&#39;</span>: n_heads <span class="op">*</span> (q_len <span class="op">*</span> kv_len <span class="op">*</span> qk_head_dim <span class="op">+</span> q_len <span class="op">*</span> kv_len <span class="op">*</span> v_head_dim), <span class="co"># MHA</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_oproj&#39;</span>: q_len <span class="op">*</span> n_heads <span class="op">*</span> v_head_dim <span class="op">*</span> hidden_size <span class="co"># o_proj</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    total_flops <span class="op">=</span> <span class="bu">sum</span>(v <span class="cf">for</span> v <span class="kw">in</span> all_ops.values())</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose <span class="kw">is</span> <span class="va">True</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;-&#39;</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Vanilla FLOPS for each step:&#39;</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> all_ops.items():</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss"> (%</span><span class="sc">{</span>v<span class="op">/</span>total_flops<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">)&#39;</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Total:&#39;</span>, total_flops)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_flops</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flops_absorb(q_len, kv_len, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    all_ops <span class="op">=</span> {</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_q_a&#39;</span>: q_len <span class="op">*</span> hidden_size <span class="op">*</span> q_lora_rank, <span class="co"># from Q to c_q</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_kv_a&#39;</span>: kv_len <span class="op">*</span> hidden_size <span class="op">*</span> (kv_lora_rank <span class="op">+</span> qk_rope_head_dim), <span class="co"># from KV to c_kv and k_pe</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_q_b&#39;</span>: q_len <span class="op">*</span> q_lora_rank <span class="op">*</span> qk_head_dim <span class="op">*</span> n_heads, <span class="co"># from c_q to q_pe</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_k_b&#39;</span>: q_len <span class="op">*</span> qk_nope_head_dim <span class="op">*</span> n_heads <span class="op">*</span> kv_lora_rank, </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_mqa&#39;</span>: n_heads <span class="op">*</span> (q_len <span class="op">*</span> kv_len <span class="op">*</span> (qk_rope_head_dim <span class="op">+</span> kv_lora_rank) <span class="op">+</span> q_len <span class="op">*</span> kv_len <span class="op">*</span> kv_lora_rank), <span class="co"># MQA</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_v_b&#39;</span>: q_len <span class="op">*</span> kv_lora_rank <span class="op">*</span> n_heads <span class="op">*</span> v_head_dim,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;flops_oproj&#39;</span>: q_len <span class="op">*</span> n_heads <span class="op">*</span> v_head_dim <span class="op">*</span> hidden_size,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    total_flops <span class="op">=</span> <span class="bu">sum</span>(v <span class="cf">for</span> v <span class="kw">in</span> all_ops.values())</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose <span class="kw">is</span> <span class="va">True</span>:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;-&#39;</span> <span class="op">*</span> <span class="dv">20</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Absorb2 FLOPS for each step:&#39;</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> all_ops.items():</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss"> (%</span><span class="sc">{</span>v<span class="op">/</span>total_flops<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">)&#39;</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Total:&#39;</span>, total_flops)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_flops</span></code></pre></div>
                <h3 id="code">Code</h3>
                <ul>
                <li><strong>对于 q 来说</strong>，首先计算
                latent，然后由 latent 计算出来 q 的 rope 和 nope 部分
                <ul>
                <li><code>qk_head_dim = qk_rope_head_dim + qk_nope_head_dim</code></li>
                <li><code>hidden_size</code> → <code>qk_lora_rank</code>
                → <code>n_heads * qk_head_dim</code></li>
                </ul></li>
                </ul>
                <div class="sourceCode" id="cb2"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">self</span>.q_lora_rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="va">self</span>.wq(x)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="va">self</span>.wq_b(<span class="va">self</span>.q_norm(<span class="va">self</span>.wq_a(x)))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>n_local_heads <span class="op">=</span> n_heads <span class="op">/</span> world_size</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> q.view(bsz, seqlen, n_local_heads, qk_head_dim)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>q_nope, q_pe <span class="op">=</span> torch.split(q, [qk_nope_head_dim, qk_rope_head_dim], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>q_pe <span class="op">=</span> apply_rope(q_pe)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.cat([q_nope, q_pe], dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div>
                <ul>
                <li><strong>对于 kv 来说</strong>
                <ul>
                <li>k 的 rope 部分直接由 x 计算得到，并且是 n_heads
                共用一个 k_rope</li>
                <li>k 的 nope 部分、v 由 kv_compressed 升维得到</li>
                </ul></li>
                </ul>
                <div class="sourceCode" id="cb3"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>kv <span class="op">=</span> <span class="va">self</span>.wkv_a(x)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>kv, k_pe <span class="op">=</span> torch.split(kv, [kv_lora_rank, qk_rope_head_dim])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>k_pe <span class="op">=</span> apply_rope(k_pe)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>k_pe <span class="op">=</span> k_pe.expand(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, n_heads, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>kv <span class="op">=</span> <span class="va">self</span>.wkv_b(<span class="va">self</span>.kv_norm(kv))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>k_nope, v <span class="op">=</span> torch.split(kv, [qk_nope_head_dim, v_head_dim])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> torch.cat([k_nope, k_pe], dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div>
                <p>对于 <strong>kv cache</strong> 保存和
                <strong>attention 计算</strong>：</p>
                <div class="sourceCode" id="cb4"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>k_cache[:bsz, start_pos:end_pos] <span class="op">=</span> k</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>v_cache[:bsz, start_pos:end_pos] <span class="op">=</span> v</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> torch.einsum(<span class="st">&quot;bshd,bthd-&gt;bsht&quot;</span>, q, k_cache[:bsz, :end_pos]) <span class="op">*</span> softmax_scale</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.einsum(<span class="st">&quot;bsth,bthd-&gt;bshd&quot;</span>, scores, <span class="va">self</span>.v_cache[:bsz, :end_pos])</span></code></pre></div>
                <p>上述是 naive attention
                的实现，问题在于仍然需要保存完整的 kv
                cache，我们只想保存 latent，所以可以将 q_nope 和 wkv_b
                的 k_nope 部分进行一个融合。</p>
                <div class="sourceCode" id="cb5"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>wkv_b <span class="op">=</span> wkv_b.view(n_local_heads, kv_lora_rank, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>q_nope <span class="op">=</span> torch.einsum(<span class="st">&quot;bshd,hdc-&gt;bshc&quot;</span>, q_nope, wkv_b[:, :qk_nope_head_dim]) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>q_absorb <span class="op">=</span> q_nope</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>kv_cache[:bsz, start_pos:end_pos] <span class="op">=</span> <span class="va">self</span>.kv_norm(kv)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>k_pe_cache[:bsz, start_pos:end_pos]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> (</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    torch.einsum(<span class="st">&quot;bshc,btc-&gt;bsht&quot;</span>, q_absorb, kv_cache[:bsz, :end_pos]) <span class="op">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    torch.einsum(<span class="st">&quot;bshr,btr-&gt;bsht&quot;</span>, q_pe, k_pe_cache[:bsz, :end_pos])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>) <span class="op">*</span> softmax_scale</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.einsum(<span class="st">&quot;bsht,btc-&gt;bshc&quot;</span>, scores, kv_cache[:bsz, :end_pos])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.einsum(<span class="st">&quot;bshc,hdc-&gt;bshd&quot;</span>, x, wkv_b[:, <span class="op">-</span>v_head_dim:])</span></code></pre></div>
              </div>
    </div>
  </div>
</body>

</html>
