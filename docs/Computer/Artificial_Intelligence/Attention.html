<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    Attention
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      <style type="text/css">
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
    </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Attention</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#mha" id="toc-mha">MHA</a></li>
          <li><a href="#mqa" id="toc-mqa">MQA</a></li>
          <li><a href="#gqa" id="toc-gqa">GQA</a></li>
          <li><a href="#code" id="toc-code">Code</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h3 id="mha">MHA</h3>
                <p>输入为矩阵 <span class="math inline">\(X \in
                \mathbb{R}^{L \times d_{\text{model}}}\)</span>，长度为
                <span class="math inline">\(L\)</span>，模型维度为 <span
                class="math inline">\(d_{\text{model}}\)</span>。</p>
                <p>我们有：</p>
                <ul>
                <li><span class="math inline">\(h\)</span>：attention
                head 的数量</li>
                <li><span class="math inline">\(d_k = d_v =
                d_{\text{model}} / h\)</span></li>
                </ul>
                <p>每个 head 有独立的投影：</p>
                <p><span class="math display">\[ Q_i = X W_i^Q,\quad K_i
                = X W_i^K,\quad V_i = X W_i^V,\quad i = 1, \dots, h
                \]</span></p>
                <p>每个 head 的 attention：</p>
                <p><span class="math display">\[ \text{head}_i =
                \text{softmax}\left( \frac{Q_i K_i^\top}{\sqrt{d_k}}
                \right) V_i \]</span></p>
                <p>最后拼接并映射输出：</p>
                <p><span class="math display">\[ \text{MHA}(X) =
                \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
                \]</span></p>
                <h3 id="mqa">MQA</h3>
                <p>MQA：Multi-Query Attention（多查询注意力）</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-22-17-17-36.png"
                width="500" alt="MQA" />
                <figcaption aria-hidden="true">MQA</figcaption>
                </figure>
                <p><strong>核心区别</strong>：<strong>所有 head
                共享同一个 K 和 V</strong>。</p>
                <p>我们仍然有：</p>
                <ul>
                <li><span class="math inline">\(Q_i = X W_i^Q \quad (i =
                1, \dots, h)\)</span></li>
                <li>但所有 head 使用同一组： <span
                class="math display">\[ K = X W^K,\quad V = X W^V
                \]</span></li>
                </ul>
                <p>那么每个 head 的 attention 变成：</p>
                <p><span class="math display">\[ \text{head}_i =
                \text{softmax}\left( \frac{Q_i K^\top}{\sqrt{d_k}}
                \right) V \]</span></p>
                <p>最后再拼接：</p>
                <p><span class="math display">\[ \text{MQA}(X) =
                \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
                \]</span></p>
                <h3 id="gqa">GQA</h3>
                <p>GQA：Grouped Query Attention（分组查询注意力）</p>
                <p>GQA 就是对 hidden dimension 进行分组，组内使用相同的
                K/V，相当于采取了 MHA 和 MQA
                的一个这种方案，对于每一组，实际上是使用 MQA。</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-22-17-26-46.png"
                width="500" alt="GQA" />
                <figcaption aria-hidden="true">GQA</figcaption>
                </figure>
                <p>假设：</p>
                <ul>
                <li><span class="math inline">\(h\)</span>：总 head
                数</li>
                <li><span class="math inline">\(g\)</span>：K/V
                分组数（每组共享一个 K/V）</li>
                <li><span class="math inline">\(h_g = h /
                g\)</span>：每组中的 head 数</li>
                </ul>
                <p>我们有：</p>
                <ul>
                <li>对每组 <span class="math inline">\(j = 1, \dots,
                g\)</span>，生成： <span class="math display">\[ K_j = X
                W_j^K,\quad V_j = X W_j^V \]</span></li>
                <li>每个 head <span class="math inline">\(i\)</span>
                属于某组 <span class="math inline">\(j = \lfloor i / h_g
                \rfloor\)</span>，该 head 用： <span
                class="math display">\[ Q_i = X W_i^Q,\quad K_j,\quad
                V_j \]</span></li>
                </ul>
                <p>对应 attention 计算为：</p>
                <p><span class="math display">\[ \text{head}_i =
                \text{softmax}\left( \frac{Q_i K_j^\top}{\sqrt{d_k}}
                \right) V_j \]</span></p>
                <p>最后一样拼接：</p>
                <p><span class="math display">\[ \text{GQA}(X) =
                \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
                \]</span></p>
                <hr />
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-22-17-29-40.png"
                width="500" alt="MHA, MQA, GQA" />
                <figcaption aria-hidden="true">MHA, MQA,
                GQA</figcaption>
                </figure>
                <table>
                <colgroup>
                <col style="width: 6%" />
                <col style="width: 10%" />
                <col style="width: 48%" />
                <col style="width: 36%" />
                </colgroup>
                <thead>
                <tr>
                <th></th>
                <th><span class="math inline">\(Q_i\)</span></th>
                <th><span class="math inline">\(K_i, V_i\)</span></th>
                <th>说明</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>MHA</td>
                <td>独立</td>
                <td>独立</td>
                <td>每个 head 有自己的一套 K/V</td>
                </tr>
                <tr>
                <td>MQA</td>
                <td>独立</td>
                <td>所有 head 共用一个 <span class="math inline">\(K,
                V\)</span></td>
                <td>显存最省，但表达力略差</td>
                </tr>
                <tr>
                <td>GQA</td>
                <td>独立</td>
                <td>每组共享 <span class="math inline">\(K_j,
                V_j\)</span>（共 <span class="math inline">\(g\)</span>
                组）</td>
                <td>折中方案，性能和表达力均衡</td>
                </tr>
                </tbody>
                </table>
                <h3 id="code">Code</h3>
                <h4 id="attention">Attention</h4>
                <p><span class="math inline">\(Q, K, V = XW\)</span></p>
                <p><span class="math inline">\(Attention(Q, K, V) =
                \text{Softmax}(\frac{Q K^{T}}{\sqrt{d_k}})
                V\)</span></p>
                <p><span class="math inline">\(Output = Attention(Q, K,
                V) W_{O}\)</span></p>
                <div class="sourceCode" id="cb1"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> einops <span class="im">import</span> rearrange</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionAISummer(nn.Module):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_qvk <span class="op">=</span> nn.Linear(dim, dim <span class="op">*</span> <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale_factor <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span>  <span class="co"># 1/np.sqrt(dim)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> x.dim() <span class="op">==</span> <span class="dv">3</span>, <span class="st">&#39;3D tensor must be provided&#39;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.to_qvk(x)  <span class="co"># [batch, tokens, dim*3 ]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decomposition to q,v,k</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> <span class="bu">tuple</span>(rearrange(qkv, <span class="st">&#39;b t (d k) -&gt; k b t d &#39;</span>, k<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resulting shape: [batch, tokens, tokens]</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        scaled_dot_prod <span class="op">=</span> torch.einsum(<span class="st">&#39;b i d , b j d -&gt; b i j&#39;</span>, q, k) <span class="op">*</span> <span class="va">self</span>.scale_factor</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> mask.shape <span class="op">==</span> scaled_dot_prod.shape[<span class="dv">1</span>:]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            scaled_dot_prod <span class="op">=</span> scaled_dot_prod.masked_fill(mask, <span class="op">-</span>np.inf)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> torch.softmax(scaled_dot_prod, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.einsum(<span class="st">&#39;b i j , b j d -&gt; b i d&#39;</span>, attention, v)</span></code></pre></div>
                <h4 id="mha-1">MHA</h4>
                <figure>
                <img src="/WikiImage/image_2024-11-04-19-49-23.png"
                width="600" alt="MultiHead Attention" />
                <figcaption aria-hidden="true">MultiHead
                Attention</figcaption>
                </figure>
                <p><span class="math inline">\(MultiHead(Q, K, V) =
                Concat(head_1, \cdots, head_h) W_{o}\)</span></p>
                <p><span class="math inline">\(\text{ where } head_{i} =
                Attention(X W_{i}^{Q}, X W_{i}^{K}, X W_{i}^{V}) =
                Attention(Q_i, K_i, V_i)\)</span></p>
                <p>在多头注意力中，训练时会计算出更多的参数，用于表示不同的投影子空间。
                但是在非多头注意力中，W
                中只有一个投影子空间，所以捕获的特征更少。</p>
                <div class="sourceCode" id="cb2"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadSelfAttentionAISummer(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, heads<span class="op">=</span><span class="dv">8</span>, dim_head<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Implementation of multi-head attention layer of the original transformer model.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">        einsum and einops.rearrange is used whenever possible</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">            dim: token&#39;s dimension, i.e. word embedding vector size</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">            heads: the number of distinct representations to learn</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">            dim_head: the dim of the head. In general dim_head&lt;dim.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">            However, it may not necessary be (dim/heads)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim_head <span class="op">=</span> (<span class="bu">int</span>(dim <span class="op">/</span> heads)) <span class="cf">if</span> dim_head <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> dim_head</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        _dim <span class="op">=</span> <span class="va">self</span>.dim_head <span class="op">*</span> heads</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> heads</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_qvk <span class="op">=</span> nn.Linear(dim, _dim <span class="op">*</span> <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_0 <span class="op">=</span> nn.Linear( _dim, dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale_factor <span class="op">=</span> <span class="va">self</span>.dim_head <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> x.dim() <span class="op">==</span> <span class="dv">3</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.to_qvk(x)  <span class="co"># [batch, tokens, dim*3*heads ]</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decomposition to q,v,k and cast to tuple</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the resulted shape before casting to tuple will be:</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [3, batch, heads, tokens, dim_head]</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> <span class="bu">tuple</span>(rearrange(qkv, <span class="st">&#39;b t (d k h) -&gt; k b h t d &#39;</span>, k<span class="op">=</span><span class="dv">3</span>, h<span class="op">=</span><span class="va">self</span>.heads))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># resulted shape will be: [batch, heads, tokens, tokens]</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        scaled_dot_prod <span class="op">=</span> torch.einsum(<span class="st">&#39;b h i d , b h j d -&gt; b h i j&#39;</span>, q, k) <span class="op">*</span> <span class="va">self</span>.scale_factor</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> mask.shape <span class="op">==</span> scaled_dot_prod.shape[<span class="dv">2</span>:]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>            scaled_dot_prod <span class="op">=</span> scaled_dot_prod.masked_fill(mask, <span class="op">-</span>np.inf)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> torch.softmax(scaled_dot_prod, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.einsum(<span class="st">&#39;b h i j , b h j d -&gt; b h i d&#39;</span>, attention, v)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> rearrange(out, <span class="st">&quot;b h t d -&gt; b t (h d)&quot;</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.W_0(out)</span></code></pre></div>
                <h4 id="intuition">Intuition</h4>
                <figure>
                <img
                src="../.././WikiImage/image_2025-02-08-17-06-30.png"
                alt="intuition" />
                <figcaption aria-hidden="true">intuition</figcaption>
                </figure>
              </div>
    </div>
  </div>
</body>

</html>
