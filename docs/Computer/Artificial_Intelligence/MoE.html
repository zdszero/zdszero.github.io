<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    MoE
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      <style type="text/css">
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
    </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">MoE</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#idea" id="toc-idea">Idea</a></li>
          <li><a href="#densesparse-layer"
          id="toc-densesparse-layer">Dense/Sparse Layer</a></li>
          <li><a href="#expert" id="toc-expert">Expert</a></li>
          <li><a href="#routing-function"
          id="toc-routing-function">Routing Function</a></li>
          <li><a href="#fine-grained-expert"
          id="toc-fine-grained-expert">Fine Grained Expert</a></li>
          <li><a href="#practice" id="toc-practice">Practice</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <p>Exploiting scale in both training data and model size
                has been central to the success of deep learning. When
                datasets are sufficiently large, increasing the capacity
                (number of parameters) of neural networks can give much
                better prediction accuracy.</p>
                <h3 id="idea">Idea</h3>
                <p>The basic idea of MoE is split the FFN into multiple
                sub-networks(experts), for each input token, only part
                of sub-networks(experts) are activated. Different
                sub-networks behavior as different “experts”, during
                training they absorb different information and knowledge
                from the dataset, during inferencing only part of
                experts are activated based on the input token.</p>
                <ul>
                <li><strong>Gating Network</strong>: This small neural
                network takes the input and learns to determine which
                experts are most relevant for processing that specific
                input. It produces scores or probabilities for each
                expert</li>
                </ul>
                <h3 id="densesparse-layer">Dense/Sparse Layer</h3>
                <h4 id="dense-layer">Dense Layer</h4>
                <p>FFNN (Feedforward Neural Network)</p>
                <p>An FFNN allows the model to use the contextual
                information created by the attention mechanism,
                transforming it further to capture more complex
                relationships in the data.</p>
                <p>MLP is a type of FFNN</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-17-40-48.png"
                width="500" alt="dense layer" />
                <figcaption aria-hidden="true">dense layer</figcaption>
                </figure>
                <h4 id="sparse-layer">Sparse Layer</h4>
                <p>Only activate a portion of the parameters</p>
                <p>Each expert learns different information during
                training</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-17-43-21.png"
                width="500" alt="sparse layer" />
                <figcaption aria-hidden="true">sparse layer</figcaption>
                </figure>
                <h3 id="expert">Expert</h3>
                <p><code>MoEGate</code>：选择专家的模块，为一个 token
                选择专家，输入是 embedding + attention。</p>
                <p>一个标准的 transformer block：</p>
                <pre><code>x ──▶ LayerNorm
     └──▶ Multi-Head Attention ───▶ residual add
                                   ↓
                             [hidden_state]
                                   ↓
                             LayerNorm
                                   ↓
                             FeedForward（原版）   
                               ↓
                             MoE（替代版）
                                   ↓
                             residual add
                                   ↓
                              output x</code></pre>
                <div class="sourceCode" id="cb2"><pre
                class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Before MoE</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># MoE input = Attention Output + Residual = hidden_state</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> x <span class="op">+</span> MultiHeadAttention(LayerNorm(x))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use MoE to replace FFN</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> FFN(LayerNorm(hidden_state))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> MoE(LayerNorm(hidden_state))</span></code></pre></div>
                <p>首先通过 MoEGate 计算所有 token 的
                logits，计算完之后通过 grouped_topk 为每一个 token
                选择若干专家。</p>
                <ul>
                <li>在 deepseek v3 中，从 256 个专家中选择 8 个</li>
                </ul>
                <h3 id="routing-function">Routing Function</h3>
                <p>Three general ideas:</p>
                <ul>
                <li>token choose experts</li>
                <li>expert choose tokens</li>
                <li>globally decide expert assignments</li>
                </ul>
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-16-09-52-13.png"
                width="500" alt="route function idea" />
                <figcaption aria-hidden="true">route function
                idea</figcaption>
                </figure>
                <p>In practice, almost all the MoEs do token chooses top
                k. So each token is going to rank order experts by
                affinity（计算每个专家的亲和度，这也是 MoEGate
                做的事情），and then there’s going to be top K
                choice.</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-16-10-01-03.png"
                width="500" alt="top k" />
                <figcaption aria-hidden="true">top k</figcaption>
                </figure>
                <p><span class="math display">\[
                \mathbf{h}_t^l = \sum_{i=1}^{N} \left( g_{i,t} \,
                \mathrm{FFN}_i\left( \mathbf{u}_t^l \right) \right) +
                \mathbf{u}_t^l,
                \]</span></p>
                <p><span class="math display">\[
                g_{i,t} =
                \begin{cases}
                s_{i,t}, &amp; \text{if } s_{i,t} \in
                \mathrm{Topk}\left(\{s_{j,t} \mid 1 \leq j \leq N\}, K
                \right), \\
                0, &amp; \text{otherwise},
                \end{cases}
                \]</span></p>
                <p><span class="math display">\[
                s_{i,t} = \mathrm{Softmax}_i\left( \left( \mathbf{u}_t^l
                \right)^\top \mathbf{e}_i^l \right)
                \]</span></p>
                <h3 id="fine-grained-expert">Fine Grained Expert</h3>
                <h4 id="标准-moe">标准 MoE</h4>
                <p>标准 MoE 做 token level routing</p>
                <ul>
                <li>输入 token 的完整 hidden vector（例如 4096
                维）会被送到 1 或几个专家。</li>
                <li>举例：一个 token 可能被分配到专家 3 和专家
                7，这两个专家分别处理整个 hidden vector。</li>
                </ul>
                <h4 id="fine-grained-expert-1">Fine-Grained Expert</h4>
                <p>Fined Grained MoE 做维度级的 routing</p>
                <ul>
                <li><p>将每个 token 的 hidden vector 拆成多个
                chunk（段），例如将 4096 拆成 4 个 1024
                维小段。</p></li>
                <li><p>然后为每段单独选择专家：</p>
                <ul>
                <li>第一个 chunk → 专家 1</li>
                <li>第二个 chunk → 专家 5</li>
                <li>第三个 chunk → 专家 1</li>
                <li>第四个 chunk → 专家 3</li>
                </ul></li>
                <li><p>类似于专家是在维度上<strong>并行拼接</strong>，而不是处理整个
                token。</p></li>
                </ul>
                <h3 id="practice">Practice</h3>
                <p>deepseek 之前大多数 MoE 时间都是选取 8-16
                个专家，然后在 forward
                的过程中激活其中两个专家，deepseek v3
                做了一个非常激进的配置，在 256 专家中激活 8 个专家。</p>
                <table>
                <thead>
                <tr>
                <th>Model</th>
                <th>Routed</th>
                <th>Active</th>
                <th>Shared</th>
                <th>Fine-grained ratio</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>GShard</td>
                <td>2048</td>
                <td>2</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>Switch Transformer</td>
                <td>64</td>
                <td>1</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>ST-MOE</td>
                <td>64</td>
                <td>2</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>Mixtral</td>
                <td>8</td>
                <td>2</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>DBRX</td>
                <td>16</td>
                <td>4</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>Grok</td>
                <td>8</td>
                <td>2</td>
                <td>0</td>
                <td></td>
                </tr>
                <tr>
                <td>DeepSeek v1</td>
                <td>64</td>
                <td>6</td>
                <td>2</td>
                <td>1/4</td>
                </tr>
                <tr>
                <td>Qwen 1.5</td>
                <td>60</td>
                <td>4</td>
                <td>4</td>
                <td>1/8</td>
                </tr>
                <tr>
                <td>DeepSeek v3</td>
                <td>256</td>
                <td>8</td>
                <td>1</td>
                <td>1/14</td>
                </tr>
                <tr>
                <td>OlMoE</td>
                <td>64</td>
                <td>8</td>
                <td>0</td>
                <td>1/8</td>
                </tr>
                <tr>
                <td>MiniMax</td>
                <td>32</td>
                <td>2</td>
                <td>0</td>
                <td>~1/4</td>
                </tr>
                </tbody>
                </table>
              </div>
    </div>
  </div>
</body>

</html>
