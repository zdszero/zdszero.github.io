<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    MoE
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">MoE</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#idea" id="toc-idea">Idea</a></li>
          <li><a href="#densesparse-layer"
          id="toc-densesparse-layer">Dense/Sparse Layer</a></li>
          <li><a href="#expert" id="toc-expert">Expert</a></li>
          <li><a href="#what-to-learn" id="toc-what-to-learn">What to
          learn</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <p>Exploiting scale in both training data and model size
                has been central to the success of deep learning. When
                datasets are sufficiently large, increasing the capacity
                (number of parameters) of neural networks can give much
                better prediction accuracy.</p>
                <h3 id="idea">Idea</h3>
                <p>The basic idea of MoE is split the FFN into multiple
                sub-networks(experts), for each input token, only part
                of sub-networks(experts) are activated. Different
                sub-networks behavior as different “experts”, during
                training they absorb different information and knowledge
                from the dataset, during inferencing only part of
                experts are activated based on the input token.</p>
                <ul>
                <li><strong>Gating Network</strong>: This small neural
                network takes the input and learns to determine which
                experts are most relevant for processing that specific
                input. It produces scores or probabilities for each
                expert</li>
                </ul>
                <h3 id="densesparse-layer">Dense/Sparse Layer</h3>
                <h4 id="dense-layer">Dense Layer</h4>
                <p>FFNN (Feedforward Neural Network)</p>
                <p>An FFNN allows the model to use the contextual
                information created by the attention mechanism,
                transforming it further to capture more complex
                relationships in the data.</p>
                <p>MLP is a type of FFNN</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-17-40-48.png"
                width="500" alt="dense layer" />
                <figcaption aria-hidden="true">dense layer</figcaption>
                </figure>
                <h4 id="sparse-layer">Sparse Layer</h4>
                <p>Only activate a portion of the parameters</p>
                <p>Each expert learns different information during
                training</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-17-43-21.png"
                width="500" alt="sparse layer" />
                <figcaption aria-hidden="true">sparse layer</figcaption>
                </figure>
                <h3 id="expert">Expert</h3>
                <h3 id="what-to-learn">What to learn</h3>
                <p><u>Mixtral Paper</u></p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-17-46-22.png"
                alt="Example Expert" />
                <figcaption aria-hidden="true">Example
                Expert</figcaption>
                </figure>
                <hr />
                <p><a
                href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">MoE
                walkthrough</a></p>
              </div>
    </div>
  </div>
</body>

</html>
