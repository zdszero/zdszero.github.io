<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    Estimation
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Estimation</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#time" id="toc-time">Time</a></li>
          <li><a href="#roofline" id="toc-roofline">Roofline</a></li>
          <li><a href="#matrix-gemm" id="toc-matrix-gemm">Matrix
          Gemm</a></li>
          <li><a href="#transformer"
          id="toc-transformer">Transformer</a></li>
          <li><a href="#mlps" id="toc-mlps">MLPs</a></li>
          <li><a href="#attention" id="toc-attention">Attention</a></li>
          <li><a href="#attn-gemm" id="toc-attn-gemm">attn &amp;
          gemm</a></li>
          <li><a href="#general-rule" id="toc-general-rule">general
          rule</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h3 id="time">Time</h3>
                <h4 id="computation">Computation</h4>
                <p>A deep learning model is effectively a bunch of
                matrix multiplications, each composed of floating-point
                multiplication and addition ‘operations’ (FLOPs). Our
                accelerator speed determines how long these take to
                compute:</p>
                <p><span class="math inline">\(T_{math} =
                \text{Computation_FLOPs} /
                \text{Accelerator_FLOPs/s}\)</span></p>
                <p>例如，NVIDIA H100 大约能达到 9.89e14 bfloat16 FLOPs/s
                的性能，而 TPU v6e 则为 9.1e14 FLOPs/s。这意味着在 H100
                上执行 1e12 FLOPs 大约需要 1e12 / 9.89e14 = <strong>1.01
                毫秒</strong>，在 TPU v6e 上则需要 1e12 / 9.1e14 =
                <strong>1.1 毫秒</strong>。</p>
                <h4 id="communication-within-a-chip">Communication
                within a chip</h4>
                <p>在 Accelerator 内部，Tensor 需要在片上存储器（HBM）和
                Tensor Core 之间进行传输。这种连接的带宽被称为“HBM
                带宽”。在 H100 上，这个带宽大约是 <strong>3.35
                TB/s</strong>，而在 TPU v6e 上，大约是 <strong>1.6
                TB/s</strong>。</p>
                <h4 id="communication-between-chips">Communication
                between chips</h4>
                <p>当我们把一个模型分布式部署到多个加速器上时，张量（数据）经常需要在它们之间进行传输。在我们的硬件上，通常有几种实现这种传输的选项（ICI、DCN
                和 PCIe），每种都有不同的带宽。</p>
                <p>不论是对于 inner chip，还是 intra
                chips，可以通过以下公式衡量时间：</p>
                <p><span class="math display">\[T_{comms} =
                \frac{\text{Communication_Bytes}}{\text{Network/Memory_Bandwidth_Bytes/s}}\]</span></p>
                <hr />
                <p><strong>计算访存比</strong>（Intensity）：</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-07-23-16-41-50.png"
                width="500" alt="Intensity" />
                <figcaption aria-hidden="true">Intensity</figcaption>
                </figure>
                <p>Intensity(Accelerator) TPU 达到其峰值 FLOPs/s
                时的算术强度。对于 TPU v5e MXU，这大约是 240
                FLOPs/B，因为 TPU 每秒可以执行 1.97e14 FLOPs 并从 HBM
                加载 8.2e11 字节/秒。这意味着如果一个算法的算术强度低于
                240
                FLOPs/字节，它将受限于字节加载，因此我们无法充分利用我们的硬件。</p>
                <p><strong>Self Dot Product</strong></p>
                <p>对于两个 vector
                的内积：<code>x * y: bf16[N] * bf16[N] → bf16[1]</code>，我们需要将
                <code>x</code> 和 <code>y</code>
                加载到内存中进行计算，每个 vector 有 2N
                的数据需要加载，进行 N 次乘法和 N-1 次加法：</p>
                <p><span class="math display">\[Intensity(dot product) =
                \frac{N + N - 1}{2N + 2N} → \frac{1}{2}\]</span></p>
                <p>当 <span class="math inline">\(N →
                \inf\)</span>，这个计算访存比为
                0.5，每加载一个字节的数据进行 0.5
                次浮点计算，这意味着该阶段的通信开销远远大于计算开销。</p>
                <h3 id="roofline">Roofline</h3>
                <h3 id="matrix-gemm">Matrix Gemm</h3>
                <p>对于以下配置：</p>
                <table>
                <thead>
                <tr>
                <th>array</th>
                <th>shape</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td><span class="math inline">\(x\)</span></td>
                <td><span class="math inline">\([P]\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(y\)</span></td>
                <td><span class="math inline">\([P]\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A\)</span></td>
                <td><span class="math inline">\([N P]\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(B\)</span></td>
                <td><span class="math inline">\([P M]\)</span></td>
                </tr>
                </tbody>
                </table>
                <ul>
                <li>两个向量 <span class="math inline">\(x \cdot
                y\)</span> 的 <strong>点积</strong> 需要 <span
                class="math inline">\(P\)</span> 次加法和乘法，总计
                <span class="math inline">\(2P\)</span>
                次浮点运算。</li>
                <li><strong>矩阵-向量积</strong> <span
                class="math inline">\(Ax\)</span> 需要对 <span
                class="math inline">\(A\)</span> 的行进行 <span
                class="math inline">\(N\)</span> 次点积，总计 <span
                class="math inline">\(2NP\)</span> 次浮点运算。</li>
                <li><strong>矩阵-矩阵积</strong> <span
                class="math inline">\(AB\)</span> 需要对 <span
                class="math inline">\(B\)</span> 的每列进行 <span
                class="math inline">\(M\)</span> 次矩阵-向量积，总计
                <span class="math inline">\(2NPM\)</span>
                次浮点运算。</li>
                <li>通常，如果我们有两个 <strong>高维数组</strong> <span
                class="math inline">\(C\)</span> 和 <span
                class="math inline">\(D\)</span>，其中一些维度是
                <strong>CONTRACTING</strong> 维度，一些是
                <strong>Batching</strong>维度。(例如，<span
                class="math inline">\(C[GHIJKL]\)</span>，<span
                class="math inline">\(D[GHMNKL]\)</span>)，那么这种收缩的浮点运算成本是
                <span class="math inline">\(C\)</span> 和 <span
                class="math inline">\(D\)</span>
                所有维度的乘积的两倍，其中批处理和收缩维度只计算一次（例如，<span
                class="math inline">\(2GHIJMNKL\)</span>）。请注意，一个维度只有在两个乘数中都出现时才进行批处理。（另请注意，如果没有收缩维度，则因子2不适用，这只是一个元素级乘积。）</li>
                </ul>
                <blockquote>
                <p>Batching Dimension</p>
                <p>Contracting Dimension</p>
                <p>Non Batching and Non Contracting Dimension</p>
                </blockquote>
                <h3 id="transformer">Transformer</h3>
                <p>首先给出一系列 dimension 的符号表示：</p>
                <table>
                <thead>
                <tr>
                <th>symbol</th>
                <th>dimension</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td><span class="math inline">\(B\)</span></td>
                <td>batch size</td>
                </tr>
                <tr>
                <td><span class="math inline">\(L\)</span></td>
                <td>number of layers</td>
                </tr>
                <tr>
                <td><span class="math inline">\(T\)</span></td>
                <td>sequence length (query)</td>
                </tr>
                <tr>
                <td><span class="math inline">\(S\)</span></td>
                <td>sequence length (key value)</td>
                </tr>
                <tr>
                <td><span class="math inline">\(V\)</span></td>
                <td>vocab</td>
                </tr>
                <tr>
                <td><span class="math inline">\(D\)</span></td>
                <td>d_model, embedding dimension, hidden size</td>
                </tr>
                <tr>
                <td><span class="math inline">\(F\)</span></td>
                <td>MLP intermidiate dimension</td>
                </tr>
                <tr>
                <td><span class="math inline">\(H\)</span></td>
                <td>attention head dimension</td>
                </tr>
                <tr>
                <td><span class="math inline">\(N\)</span></td>
                <td>number of query heads</td>
                </tr>
                <tr>
                <td><span class="math inline">\(K\)</span></td>
                <td>number of key/value heads</td>
                </tr>
                <tr>
                <td><span class="math inline">\(G\)</span></td>
                <td>q heads per kv head = <span class="math inline">\(N
                // K\)</span></td>
                </tr>
                </tbody>
                </table>
                <figure>
                <img
                src="https://jax-ml.github.io/scaling-book/assets/img/transformer-diagram.png"
                alt="Transformer" />
                <figcaption aria-hidden="true">Transformer</figcaption>
                </figure>
                <p><strong>为什么要区分 K/N</strong></p>
                <ul>
                <li>对于 MHA，K 和 N 相同</li>
                <li>对于 MQA，K = 1</li>
                <li>对于 GQA，N 是 K 的整数倍，G = N / K</li>
                </ul>
                <p><strong>gating einsum</strong></p>
                <p>这是一段关于“门控爱因斯坦求和”（gating
                einsum）的解释，以下是翻译内容：</p>
                <p>我们将上投影矩阵分成两个矩阵（上面的 <span
                class="math inline">\(W\_{In1}\)</span> 和 <span
                class="math inline">\(W\_{In2}\)</span>），它们的输出进行逐元素相乘，作为一种“门控函数”。并非所有大型语言模型（LLM）都使用这种方法，所以有时你会看到一个单独的
                <span class="math inline">\(W\_{In}\)</span>
                矩阵，并且总的多层感知器（MLP）参数计数是 2DF 而不是
                3DF。通常在这种情况下，D 和 F
                会按比例放大，以保持参数计数与三个矩阵的情况相同。尽管如此，Llama、DeepSeek
                和许多其他模型都使用了某种形式的门控爱因斯坦求和。</p>
                <h3 id="mlps">MLPs</h3>
                <p><strong>Gated MLP</strong></p>
                <p><span class="math display">\[ A = \sigma(X W_{in1})
                \odot (X W_{in2}) \]</span></p>
                <p>解释：</p>
                <ul>
                <li><span class="math inline">\(W_{in1},
                W_{in2}\)</span> 都是从输入维度 <span
                class="math inline">\(D\)</span> 投影到扩展维度 $F$
                的两个不同矩阵</li>
                <li>第一个投影 <span class="math inline">\(X
                W_{in1}\)</span> 通过 sigmoid 激活变成门控值</li>
                <li>第二个投影 <span class="math inline">\(X
                W_{in2}\)</span> 作为被门控的主流路径</li>
                <li>二者逐元素相乘之后再通过 <span
                class="math inline">\(W_{out}\)</span> 降维</li>
                </ul>
                <p>这种结构比标准 MLP 更灵活，最早用于 GLU，也出现在
                Gated MLP, SwiGLU, GeGLU 等设计中。</p>
                <table style="width:100%;">
                <colgroup>
                <col style="width: 65%" />
                <col style="width: 23%" />
                <col style="width: 10%" />
                </colgroup>
                <thead>
                <tr>
                <th>operation</th>
                <th>train FLOPs</th>
                <th>params</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td><span class="math inline">\(A[B, T,
                \textcolor{red}{D}] \cdot W_{in1}[\textcolor{red}{D},
                F]\)</span></td>
                <td><span class="math inline">\(6BTDF\)</span></td>
                <td><span class="math inline">\(DF\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A[B, T,
                \textcolor{red}{D}] \cdot W_{in2}[\textcolor{red}{D},
                F]\)</span></td>
                <td><span class="math inline">\(6BTDF\)</span></td>
                <td><span class="math inline">\(DF\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(\sigma(A_{in1})[B, T, F]
                * A_{in2}[B, T, F]\)</span></td>
                <td><em><span
                class="math inline">\(O(BTF)\)</span></em></td>
                <td></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A[B, T, F] \cdot
                W_{out}[F, D]\)</span></td>
                <td><span class="math inline">\(6BTDF\)</span></td>
                <td><span class="math inline">\(DF\)</span></td>
                </tr>
                <tr>
                <td></td>
                <td><span class="math inline">\(\approx
                18BTDF\)</span></td>
                <td><span class="math inline">\(3DF\)</span></td>
                </tr>
                </tbody>
                </table>
                <h3 id="attention">Attention</h3>
                <table>
                <colgroup>
                <col style="width: 54%" />
                <col style="width: 25%" />
                <col style="width: 20%" />
                </colgroup>
                <thead>
                <tr>
                <th>operation</th>
                <th>train FLOPs</th>
                <th>params</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td><span class="math inline">\(A[B, T,
                \textcolor{red}{D}] \cdot W_Q[\textcolor{red}{D}, N,
                H]\)</span></td>
                <td><span class="math inline">\(6BTDNH\)</span></td>
                <td><span class="math inline">\(DNH\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A[B, T,
                \textcolor{red}{D}] \cdot W_K[\textcolor{red}{D}, K,
                H]\)</span></td>
                <td><span class="math inline">\(6BTDKH\)</span></td>
                <td><span class="math inline">\(DKH\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A[B, T,
                \textcolor{red}{D}] \cdot W_V[\textcolor{red}{D}, K,
                H]\)</span></td>
                <td><span class="math inline">\(6BTDKH\)</span></td>
                <td><span class="math inline">\(DKH\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(A[B, T, N, H] \cdot
                W_O[N, H, D]\)</span></td>
                <td><span class="math inline">\(6BTDNH\)</span></td>
                <td><span class="math inline">\(DNH\)</span></td>
                </tr>
                <tr>
                <td></td>
                <td><span class="math inline">\(\mathbf{12BTD(N +
                K)H}\)</span></td>
                <td><span class="math inline">\(\mathbf{2D(N +
                K)H}\)</span></td>
                </tr>
                </tbody>
                </table>
                <table>
                <colgroup>
                <col style="width: 60%" />
                <col style="width: 40%" />
                </colgroup>
                <thead>
                <tr>
                <th>operation</th>
                <th>train FLOPs</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td><span class="math inline">\(Q[B, T,
                \textcolor{blue}{K}, G, \textcolor{red}{H}] \cdot K[B,
                S, \textcolor{blue}{K},
                \textcolor{red}{H}]\)</span></td>
                <td><span class="math inline">\(6BTSKGH =
                6BTSNH\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(\text{softmax}_S\quad
                L[B, T, S, K, G]\)</span></td>
                <td><span class="math inline">\(O(BTSKG) =
                O(BTSN)\)</span></td>
                </tr>
                <tr>
                <td><span class="math inline">\(S[B, T,
                \textcolor{red}{S}, \textcolor{blue}{K}, G] \cdot V[B,
                \textcolor{red}{S}, \textcolor{blue}{K},
                H]\)</span></td>
                <td><span class="math inline">\(6BTSKGH =
                6BTSNH\)</span></td>
                </tr>
                <tr>
                <td></td>
                <td><span class="math inline">\(\approx 12BTSNH =
                12BT^2NH\)</span></td>
                </tr>
                </tbody>
                </table>
                <h3 id="attn-gemm">attn &amp; gemm</h3>
                <p>Suppose <span class="math inline">\(F =
                4D\)</span>，<span class="math inline">\(N =
                K\)</span>：</p>
                <p><span class="math display">\[\frac{\text{attention
                FLOPs}}{\text{matmul FLOPs}} = \frac{12BT^2NH}{18BTDF +
                24BTDNH} = \frac{12BT^2D}{4 * 18BTD^2 + 24BTD^2} =
                \frac{12BT^2D}{96BTD^2} = \frac{T}{8D}\]</span></p>
                <p>所以只有当 <span class="math inline">\(T &gt;
                8D\)</span> 的时候 attention 的计算量才会超过 gemm，假设
                hidden size = 8K，只有上下文长度达到 64k 时 attention
                计算量才会超过 gemm。</p>
                <h3 id="general-rule">general rule</h3>
                <p>如果上下文比较短，然后忽略 attention 阶段的 self dot
                product 的计算量的话，那么计算量可以近似为：</p>
                <p><span class="math display">\[
                \begin{align*}
                (18BTDF + 12BTD(N+K)H)L = 6 * BT * (3DF + 2D(N+K)H)L \\
                    = 6 * \text{num tokens} * \text{parameter count}
                \end{align*}
                \]</span></p>
                <p>这引出了一个著名的经验法则，用于估算密集型
                Transformer
                的浮点运算数（FLOP），同时忽略了注意力机制的浮点运算。
                （去嵌入（Unembedding）是另一个简单的矩阵乘法，有 <span
                class="math inline">\(6BSDV\)</span> 浮点运算和 <span
                class="math inline">\(DV\)</span>
                参数，也遵循相同的经验法则。）</p>
              </div>
    </div>
  </div>
</body>

</html>
