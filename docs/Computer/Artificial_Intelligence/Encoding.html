<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    Encoding
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Encoding</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#absolute" id="toc-absolute">Absolute</a></li>
          <li><a href="#rope" id="toc-rope">Rope</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h3 id="absolute">Absolute</h3>
                <p>Before introducing RoPE, let’s recap the basics of
                the attention mechanism. Attention focuses on pair-wise
                relationships: there’s a query vector q from one token
                and a key vector k from another. <strong>We obtain the
                attention score by taking the inner product of q and k,
                and this inner product is key to how position embeddings
                function.</strong></p>
                <p>For example, to get the attention score for the pair
                (1, 3), we get the query vector from token 1 and the key
                vector from token 3.</p>
                <p>The authors then reflect on this formulation and
                realize that in this setup, <strong>the relative
                positional information is encoded before the inner
                product — meaning it’s inherently tied to the token
                embedding.</strong></p>
                <p>They ask themselves: “Is there another way to encode
                relative positional information only when we need the
                attention score — i.e., at the moment we perform the q,k
                inner product?” Or equivalently, the
                <strong>q,k</strong> inner product is <strong>equivalent
                to another function g that takes only the token
                embeddings and their positions as input?</strong></p>
                <h3 id="rope">Rope</h3>
                <p>In simple terms, this means that after the
                transformation, we can either rotate first and then
                perform the inner product, or we can perform the inner
                product first and then rotate, and take the real part.
                In the second approach, we only need (m–n) for the
                rotation, which signifies that this is a type of
                relative position embedding.</p>
                <p><span class="math display">\[
                R^d_{\Theta,m} = \begin{pmatrix}
                \cos m\theta_1 &amp; -\sin m\theta_1 &amp; 0 &amp; 0
                &amp; \cdots &amp; 0 &amp; 0 \\
                \sin m\theta_1 &amp; \cos m\theta_1 &amp; 0 &amp; 0
                &amp; \cdots &amp; 0 &amp; 0 \\
                0 &amp; 0 &amp; \cos m\theta_2 &amp; -\sin m\theta_2
                &amp; \cdots &amp; 0 &amp; 0 \\
                0 &amp; 0 &amp; \sin m\theta_2 &amp; \cos m\theta_2
                &amp; \cdots &amp; 0 &amp; 0 \\
                \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;
                \ddots &amp; \vdots &amp; \vdots \\
                0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos
                m\theta_{d/2} &amp; -\sin m\theta_{d/2} \\
                0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin
                m\theta_{d/2} &amp; \cos m\theta_{d/2}
                \end{pmatrix}
                \]</span></p>
                <p><span class="math display">\[\theta_{i} =
                10000^{-2i/d}\]</span></p>
                <p><span
                class="math display">\[f_{\{q,k\}}(\mathbf{x}_m, m) =
                \mathbf{R}^d_{\Theta,m}
                \mathbf{W}_{\{q,k\}}\mathbf{x}_m\]</span></p>
                <p><span class="math display">\[
                \mathbf{R}^d_{\Theta,m} \mathbf{x} =
                \begin{pmatrix}
                x_1 \\
                x_2 \\
                x_3 \\
                x_4 \\
                \vdots \\
                x_{d-1} \\
                x_d
                \end{pmatrix}
                \otimes
                \begin{pmatrix}
                \cos m\theta_1 \\
                \cos m\theta_1 \\
                \cos m\theta_2 \\
                \cos m\theta_2 \\
                \vdots \\
                \cos m\theta_{d/2} \\
                \cos m\theta_{d/2}
                \end{pmatrix}
                +
                \begin{pmatrix}
                -x_2 \\
                x_1 \\
                -x_4 \\
                x_3 \\
                \vdots \\
                -x_d \\
                x_{d-1}
                \end{pmatrix}
                \otimes
                \begin{pmatrix}
                \sin m\theta_1 \\
                \sin m\theta_1 \\
                \sin m\theta_2 \\
                \sin m\theta_2 \\
                \vdots \\
                \sin m\theta_{d/2} \\
                \sin m\theta_{d/2}
                \end{pmatrix}
                \]</span></p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-05-21-14-37-42.png"
                width="800" alt="RoPE" />
                <figcaption aria-hidden="true">RoPE</figcaption>
                </figure>
                <p>The objective of RoPE:</p>
                <p><span class="math display">\[
                \left\langle f_q(\mathbf{x}_m, \mathbf{m}),
                f_k(\mathbf{x}_n, \mathbf{n}) \right\rangle =
                g(\mathbf{x}_m, \mathbf{x}_n, \mathbf{m} - \mathbf{n}).
                \]</span></p>
                <p><span class="math display">\[
                \begin{align*}
                f_q(\mathbf{x}_m, m) &amp;=
                (\mathbf{W}_q\mathbf{x}_m)e^{im\theta} \\
                f_k(\mathbf{x}_n, n) &amp;=
                (\mathbf{W}_k\mathbf{x}_n)e^{in\theta} \\
                g(\mathbf{x}_m, \mathbf{x}_n, m-n) &amp;=
                \text{Re}[(\mathbf{W}_q\mathbf{x}_m)(\mathbf{W}_k\mathbf{x}_n)^*
                e^{i(m-n)\theta}]
                \end{align*}
                \]</span></p>
              </div>
    </div>
  </div>
</body>

</html>
