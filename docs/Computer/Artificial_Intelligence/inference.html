<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    inference
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">inference</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#parallelism"
          id="toc-parallelism">Parallelism</a>
          <ul>
          <li><a href="#model-parallelism"
          id="toc-model-parallelism">Model Parallelism</a></li>
          <li><a href="#dp" id="toc-dp">DP</a></li>
          <li><a href="#tp" id="toc-tp">TP</a></li>
          <li><a href="#pp" id="toc-pp">PP</a></li>
          <li><a href="#ep" id="toc-ep">EP</a></li>
          </ul></li>
          <li><a href="#batching" id="toc-batching">Batching</a>
          <ul>
          <li><a href="#metrics" id="toc-metrics">metrics</a></li>
          </ul></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h2 id="parallelism">Parallelism</h2>
                <table>
                <colgroup>
                <col style="width: 20%" />
                <col style="width: 13%" />
                <col style="width: 17%" />
                <col style="width: 15%" />
                <col style="width: 20%" />
                <col style="width: 13%" />
                </colgroup>
                <thead>
                <tr>
                <th>Parallelism</th>
                <th>batch dim</th>
                <th>sequence dim</th>
                <th>hidden dim</th>
                <th>weights</th>
                <th>optimizer</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>DP</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                </tr>
                <tr>
                <td>TP</td>
                <td></td>
                <td></td>
                <td>√</td>
                <td>√(intra-layer)</td>
                <td></td>
                </tr>
                <tr>
                <td>Ring Attention</td>
                <td></td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                </tr>
                <tr>
                <td>PP</td>
                <td></td>
                <td></td>
                <td></td>
                <td>√(intra-layer)</td>
                <td>√</td>
                </tr>
                <tr>
                <td>ZeRO</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td></td>
                <td>√</td>
                </tr>
                <tr>
                <td>FSDP</td>
                <td>√</td>
                <td></td>
                <td></td>
                <td>√(intra-layer)</td>
                <td>√</td>
                </tr>
                </tbody>
                </table>
                <h3 id="model-parallelism">Model Parallelism</h3>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-09-17-22-43.png"
                width="500" alt="intra vs inter model parallelism" />
                <figcaption aria-hidden="true">intra vs inter model
                parallelism</figcaption>
                </figure>
                <ul>
                <li><strong>Intra-Operator</strong>: partitions
                computationally intensive operators, such as matrix
                multiplications, across multiple GPUs, accelerating
                computation but causing substantial communication.</li>
                <li><strong>Inter-Operator</strong>: organizes LLM
                layers into stages, each running on a GPU to form
                pipelines.</li>
                </ul>
                <h3 id="dp">DP</h3>
                <p>Data Parallelism is most common due to its
                simplicity:</p>
                <p>the dataset is split into several shards, each shard
                is allocated to a device.</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-24-59.png"
                width="500" alt="Data Parallism" />
                <figcaption aria-hidden="true">Data
                Parallism</figcaption>
                </figure>
                <h3 id="tp">TP</h3>
                <p>Tensor Parallelism:</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-05-38.png"
                width="500" alt="Column-wise parallelism" />
                <figcaption aria-hidden="true">Column-wise
                parallelism</figcaption>
                </figure>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-07-53.png"
                width="500" alt="Row-wise parallelism" />
                <figcaption aria-hidden="true">Row-wise
                parallelism</figcaption>
                </figure>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-08-45.png"
                width="500" alt="Combined parallelism" />
                <figcaption aria-hidden="true">Combined
                parallelism</figcaption>
                </figure>
                <p>Collective communications involve network-intensive
                operations are required after the operation.</p>
                <p>Operations</p>
                <ul>
                <li>AG (AllGather): column-wise</li>
                <li>AR (AllReduce): row-wise</li>
                </ul>
                <h3 id="pp">PP</h3>
                <p>the model is split by layer into several chunks, each
                chunk is given to a device</p>
                <p>During the forward pass, each device passes the
                intermediate activation to the next stage. During the
                backward pass, each device passes the gradient of the
                input tensor back to the previous pipeline stage.</p>
                <p>Device with PP operates on
                <strong>mirco-batch</strong> split by stages.</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-01-07-16-28-01.png"
                width="500" alt="gpipe" />
                <figcaption aria-hidden="true">gpipe</figcaption>
                </figure>
                <h3 id="ep">EP</h3>
                <p>Expert Parallism</p>
                <h2 id="batching">Batching</h2>
                <figure>
                <img
                src="../.././WikiImage/image_2024-12-25-17-30-18.png"
                width="500" alt="Batching" />
                <figcaption aria-hidden="true">Batching</figcaption>
                </figure>
                <ul>
                <li><strong>Continuous Batching</strong>: continuous and
                dynamic gpu memory</li>
                <li><strong>Static Batching</strong>: create static gpu
                memory for each request</li>
                </ul>
                <p>batch size: how many user inputs are processed
                concurrently in the LLM.</p>
                <h3 id="metrics">metrics</h3>
                <figure>
                <img
                src="../.././WikiImage/image_2024-12-25-16-56-19.png"
                width="500"
                alt="Key Metrics for evaluating the performance of LLMs" />
                <figcaption aria-hidden="true">Key Metrics for
                evaluating the performance of LLMs</figcaption>
                </figure>
                <ul>
                <li>ttft (Time to First Token)</li>
                <li>tpot (Time Per Token Output)</li>
                <li>e2el (End to End Latency)</li>
                <li>MBU (Model Bandwidth Utilization):
                <ul>
                <li>(achieved memory bandwidth) / (peak memory
                bandwidth)</li>
                <li>achieved memory bandwidth = ((total model parameter
                size + KV cache size) / TPOT)</li>
                </ul></li>
                </ul>
                <hr />
                <p><a
                href="https://colossalai.org/docs/concepts/paradigms_of_parallelism/">Paradigm
                of Parallism</a></p>
              </div>
    </div>
  </div>
</body>

</html>
