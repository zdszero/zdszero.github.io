<!doctype html>
<html >

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/template.css" />

  <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.cookie.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.hoverIntent.minified.js"></script>
  <script type='text/javascript' src="../../WikiTheme/menu/js/jquery.dcjqaccordion.2.7.min.js"></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/jquery.sticky-kit.js "></script>
  <script type="text/javascript" src="../../WikiTheme/menu/js/sticky_menu.js"></script>

  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/blue.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/graphite.css"/>
  <link rel="stylesheet" type="text/css" href="../../WikiTheme/menu/css/skins/grey.css"/>

  <meta name="generator" content="pandoc" />
      <title>
    Deepseek
  </title>
    <link rel="stylesheet" href="../../WikiTheme/theme/bootstrap.css"  />
        <script
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        type="text/javascript"></script>
      </head>

<body>
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Deepseek</span>
        <ul class="nav pull-right doc-info">
          <p class="navbar-text">
                                                      </p>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">
          <ul>
          <li><a href="#mla" id="toc-mla">MLA</a></li>
          <li><a href="#moe" id="toc-moe">MOE</a></li>
          </ul>
        </div>
      </div>
            <div class="span9">
                <h3 id="mla">MLA</h3>
                <h4 id="traditional-mhsa">Traditional MHSA</h4>
                <p>The computing process:</p>
                <p><span class="math display">\[\boldsymbol{q}_t = W^{Q}
                \cdot \boldsymbol{h}_t\]</span></p>
                <p><span class="math display">\[\boldsymbol{k}_t = W^{K}
                \cdot \boldsymbol{h}_t\]</span></p>
                <p><span class="math display">\[\boldsymbol{v}_t = W^{V}
                \cdot \boldsymbol{h}_t\]</span></p>
                <p><span class="math display">\[\boldsymbol{q}_t =
                [\boldsymbol{q}_{t,1}, \boldsymbol{q}_{t,2}, \cdots,
                \boldsymbol{q}_{t,n_h}]\]</span></p>
                <p><span class="math display">\[\boldsymbol{k}_t =
                [\boldsymbol{k}_{t,1}, \boldsymbol{k}_{t,2}, \cdots,
                \boldsymbol{k}_{t,n_h}]\]</span></p>
                <p><span class="math display">\[\boldsymbol{v}_t =
                [\boldsymbol{v}_{t,1}, \boldsymbol{v}_{t,2}, \cdots,
                \boldsymbol{v}_{t,n_h}]\]</span></p>
                <p><span class="math display">\[\boldsymbol{o}_{t, i} =
                \sum_{j=1}^{t} \text{Softmax}
                \frac{\boldsymbol{q}_{t,i}^T \cdot
                \boldsymbol{k}_{j,i}}{\sqrt{d_h}}
                \boldsymbol{v}_{j,i}\]</span></p>
                <p><span class="math display">\[\boldsymbol{u}_t = W^{O}
                [\boldsymbol{o}_{t,1}, \boldsymbol{o}_{t,2}, \cdots,
                \boldsymbol{o}_{t,n_h}]\]</span></p>
                <p><strong>dimensions</strong></p>
                <p><span class="math display">\[W^Q, W^K, W^V \in
                \mathbb{R}^{d_n n_h \times d}\]</span></p>
                <p><span class="math display">\[W^O \in \mathbb{R}^{d
                \times d_h n_h}\]</span></p>
                <p><strong>explanation</strong></p>
                <p><span class="math inline">\(\boldsymbol{h_t}\)</span>
                is attention input</p>
                <h4 id="multi-latent-attention">Multi Latent
                Attention</h4>
                <p><span class="math display">\[\boldsymbol{c}_{t}^{KV}
                = W^{DKV} \boldsymbol{h}_t\]</span></p>
                <p><span class="math display">\[\boldsymbol{k}^{C} =
                W^{UK} \boldsymbol{c}_{t}^{KV}\]</span></p>
                <p><span class="math display">\[\boldsymbol{v}^{C} =
                W^{UV} \boldsymbol{c}_{t}^{KV}\]</span></p>
                <p>Explanation:</p>
                <ul>
                <li><span class="math inline">\(c_{t}^{KV}\)</span> is
                the latent vector</li>
                <li><span class="math inline">\(W^{DKV}\)</span> is the
                compressing matrix that maps <span
                class="math inline">\(h_t\)</span> dimension from <span
                class="math inline">\((h_n \cdot d_n)\)</span> into
                <span class="math inline">\(d_c\)</span></li>
                <li><span class="math inline">\(W^{UK}\)</span> and
                <span class="math inline">\(W^{UV}\)</span> are
                up-projection matrices that map the latent vector back
                to the high-dimensional space</li>
                </ul>
                <p><strong>With RoPE</strong></p>
                <p><span class="math display">\[\boldsymbol{c}_{t}^{Q} =
                W^{DQ} \boldsymbol{h}_t\]</span></p>
                <p><span
                class="math display">\[[\boldsymbol{q}_{t,1}^{C};
                \boldsymbol{q}_{t,2}^{C}; \cdots;
                \boldsymbol{q}_{t,n_h}^{C}] = \boldsymbol{q}_{t}^{C} =
                W^{UQ} \boldsymbol{c}_{t}^{Q}\]</span></p>
                <p><span
                class="math display">\[[\boldsymbol{q}_{t,1}^{R};
                \boldsymbol{q}_{t,2}^{R}; \cdots;
                \boldsymbol{q}_{t,n_h}^{R}] = \boldsymbol{q}_{t}^{R} =
                \text{RoPE}(W^{QR} \boldsymbol{c}_{t}^{Q})\]</span></p>
                <p><span class="math display">\[\boldsymbol{q}_{t,i} =
                [\boldsymbol{q}_{t,i}^{C};
                \boldsymbol{q}_{t,i}^{R}]\]</span></p>
                <hr />
                <p><span class="math display">\[\boldsymbol{c}_{t}^{KV}
                = W^{DKV} \boldsymbol{h}_t\]</span></p>
                <p><span
                class="math display">\[[\boldsymbol{k}_{t,1}^{C};
                \boldsymbol{k}_{t,2}^{C}; \cdots;
                \boldsymbol{k}_{t,n_h}^{C}] = \boldsymbol{k}_{t}^{C} =
                W^{UK} \boldsymbol{c}_{t}^{KV}\]</span></p>
                <p><span class="math display">\[\boldsymbol{k}_{t}^{R} =
                \text{RoPE}(W^{KR} \boldsymbol{h}_{t})\]</span></p>
                <p><span class="math display">\[\boldsymbol{k}_{t,i} =
                [\boldsymbol{k}_{t,i}^{C};
                \boldsymbol{k}_{t}^{R}]\]</span></p>
                <hr />
                <p><span
                class="math display">\[[\boldsymbol{v}_{t,1}^{C};
                \boldsymbol{v}_{t,2}^{C}; \cdots;
                \boldsymbol{v}_{t,n_h}^{C}] = \boldsymbol{v}_{t}^{C} =
                W^{UV} \boldsymbol{c}_{t}^{KV}\]</span></p>
                <p><span class="math display">\[\boldsymbol{o}_{t,i} =
                \sum_{j=1}^{t}
                \text{Softmax}_{j}(\frac{\boldsymbol{q}_{t,i}^{T}
                \boldsymbol{k}_{j,i}}{\sqrt{d_h + d_h^{R}}}
                \boldsymbol{v}_{j,i}^{C})\]</span></p>
                <p><span class="math display">\[\boldsymbol{u}_{t} =
                W^{O}[\boldsymbol{o}_{t,1}; \boldsymbol{o}_{t,2};
                \cdots; \boldsymbol{o}_{t,n_h}]\]</span></p>
                <h3 id="moe">MOE</h3>
                <figure>
                <img
                src="../.././WikiImage/image_2025-06-10-15-55-15.png"
                width="500" alt="layers" />
                <figcaption aria-hidden="true">layers</figcaption>
                </figure>
                <p>routing top k 选择 8 个专家，dispatch
                分发，计算完成后 combine 组合一个输出。</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-06-10-15-57-07.png"
                width="500" alt="TP EP" />
                <figcaption aria-hidden="true">TP EP</figcaption>
                </figure>
                <p>在 TP 中，每个专家被拆得很细，计算 MFU 不够高。</p>
                <p>EP 实现专家更加集中，计算密度更高。</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-06-10-15-59-55.png"
                width="500" alt="Export Process" />
                <figcaption aria-hidden="true">Export
                Process</figcaption>
                </figure>
                <p>Dispatch All2All 和 Combine All2All 虽然叫做
                All2All，但是在 deepseek moe 中的语义并不是完全的
                all2all。</p>
                <p>因为它并不需要将 tokens 传输到所有的 dp 上。</p>
                <p>Dispatch 走的 FP8，Combine 走的 BF16 传输。</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-06-10-16-08-52.png"
                width="500" alt="传统 All2All 不足" />
                <figcaption aria-hidden="true">传统 All2All
                不足</figcaption>
                </figure>
                <p>机内冗余、卡内冗余（多个 EP 在一张卡中）</p>
                <figure>
                <img
                src="../.././WikiImage/image_2025-06-10-16-09-44.png"
                width="500" alt="网络传输过程" />
                <figcaption aria-hidden="true">网络传输过程</figcaption>
                </figure>
                <ol type="1">
                <li>SM把数据写入显存</li>
                <li>SM通知Proxy线程有数据要发送</li>
                <li>CPU发现有数据要发送</li>
                <li>CPU生成网卡发送请求WQE</li>
                <li>CPU写内存中的Doorbell Record</li>
                <li>CPU向网卡发起Doorbell</li>
                <li>网卡读取内存中的WQE</li>
                <li>网卡根据WQE从显存中读数据(GDR)</li>
                <li>网卡把数据发送到网络上</li>
                <li>网卡发送完毕把completion写入内存</li>
                <li>CPU确认发送完成回收资源</li>
                </ol>
                <p>Decode 和 Prefill 在 EP 通信上的差别：</p>
                <table>
                <thead>
                <tr>
                <th></th>
                <th>Prefill</th>
                <th>Decode</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                <td>规模</td>
                <td>32</td>
                <td>320/144</td>
                </tr>
                <tr>
                <td>平均每个 Node 激活专家数</td>
                <td>2</td>
                <td>0.2/0.44</td>
                </tr>
                <tr>
                <td>batch size</td>
                <td>16k</td>
                <td>256</td>
                </tr>
                <tr>
                <td>平均每 rank 分配 token 数</td>
                <td>500</td>
                <td>0.8/1.78</td>
                </tr>
                <tr>
                <td>时长要求</td>
                <td>6-8ms</td>
                <td>100-350us</td>
                </tr>
                </tbody>
                </table>
                <p>Decode中EP通信的实现：</p>
                <ul>
                <li>不使用两阶段
                <ul>
                <li>EP规模大，两阶段几乎无收益</li>
                <li>两阶段需要复杂元数据交互</li>
                </ul></li>
                <li>Token粒度发送
                <ul>
                <li>bs小，无法chunk聚合</li>
                <li>CPU必然成为瓶颈，需要IBGDA</li>
                </ul></li>
                </ul>
              </div>
    </div>
  </div>
</body>

</html>
